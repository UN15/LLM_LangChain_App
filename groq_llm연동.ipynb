{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148076c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a9d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae6c7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ] #human으로 해도 됨됨\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"LangServe는는 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815f9447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001DCF2D0E6C0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001DCF2D0E000> root_client=<openai.OpenAI object at 0x000001DCF2D0F770> root_async_client=<openai.AsyncOpenAI object at 0x000001DCF2D0E8A0> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    # api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model = \"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(response)\n",
    "    print(type(response.content))\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aea9bd",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a3a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 설명해주세요.\n",
    "    \"\"\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9aa7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain 연결 (LCEL) prompt+llm 연결\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95482b67",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM + OutputParser을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a18c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain 연결 (LCEL) prompt + llm + outputparser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02736518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. 컴퓨터가 데이터를 통해 스스로 학습할 수 있도록 하는 것입니다.\n",
      "\n",
      "예를 들어, 고양이와 강아지의 사진을 분류하는 모델을 만든다고 가정해 보겠습니다. 이 모델에게 고양이와 강아지의 사진을 여러 장 보여 주고, 어떤 사진은 고양이, 어떤 사진은 강아지라고 알려줍니다.\n",
      "\n",
      "모델은 처음에 고양이와 강아지의 특징을 모르기 때문에 사진을 분류하지 못합니다. 하지만 사진을 계속 보여 주고, 고양이 또는 강아지라고 알려주면 모델은 사진을 분석하여 고양이와 강아지의 특징을 스스로 학습합니다.\n",
      "\n",
      "예를 들어, 고양이는 귀가 뾰족하고, 눈이 크며, 강아지는 귀가 쳐져 있고, 꼬리가 길다는 등의 특징을 학습합니다. 이렇게 학습한 특징을 바탕으로 새로운 사진을 보여주면 모델은 스스로 고양이인지 강아지인지를 분류할 수 있습니다.\n",
      "\n",
      "이러한 학습 과정을 통해 인공지능 모델은 점점 더 정확하게 데이터를 분류하고 예측할 수 있게 됩니다.\n",
      "\n",
      "구체적으로는 다음과 같은 과정으로 학습합니다.\n",
      "\n",
      "1.  **데이터 수집**: 인공지능 모델을 학습시키기 위해 필요한 데이터를 수집합니다.\n",
      "2.  **데이터 전처리**: 수집한 데이터를 깨끗하고, 일관된 형식으로 가공합니다.\n",
      "3.  **모델 훈련**: 데이터를 바탕으로 모델을 훈련합니다. 이 과정에서 모델은 데이터의 패턴과 특징을 학습합니다.\n",
      "4.  **모델 평가**: 훈련된 모델의 성능을 평가합니다. 이를 통해 모델의 정확도와 신뢰도를 측정합니다.\n",
      "5.  **모델 개선**: 모델의 성능을 개선하기 위해 추가적인 훈련과 평가를 반복합니다.\n",
      "\n",
      "이러한 과정을 통해 인공지능 모델은 다양한 작업에 활용될 수 있습니다. 예를 들어, 이미지 분류, 자연어 처리, 음성 인식 등 다양한 분야에서 활용됩니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44a2a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LangChain은 인공지능 모델을 활용하여 자연어 처리 및 생성 작업을 지원하는 플랫폼입니다. LangChain의 주요 제품은 다음과 같습니다:\n",
      "\n",
      "1. **LangSmith**: 랭스미스는 랭체인 플랫폼의 중심 제품 중 하나로, 개발자가 자연어 처리(NLP) 모델을 쉽게 구축, 테스트 및 배포할 수 있도록 지원하는 도구입니다. 랭스미스를 통해 개발자는 사전 구축된 다양한 NLP 모델과 컴포넌트를 활용하여 애플리케이션을 개발할 수 있습니다.\n",
      "\n",
      "2. **LangChain**: 랭체인은 자체적으로도 하나의 제품으로 간주될 수 있습니다. 이는 개발자가 AI 모델을 체인처럼 연결하여 복잡한 작업을 수행할 수 있는 프레임워크를 제공합니다. 랭체인을 통해 개발자는 다양한 NLP 작업들을 하나의 워크플로우로 만들어 효율적인 처리가 가능합니다.\n",
      "\n",
      "3. **LangServe**: 랭서브는 LangChain에서 제공하는 또 다른 중요한 제품입니다. 이는 랭체인에서 개발된 애플리케이션과 모델을 쉽고 빠르게 배포하고 관리할 수 있도록 도와주는 서비스입니다. 랭서브를 통해 사용자는 모델의 성능을 모니터링하고, 필요에 따라 모델을 업데이트하거나 확장할 수 있습니다.\n",
      "\n",
      "이러한 제품들은 LangChain이 목표로 하는 사용자 친화적인 AI 개발 환경의 일부입니다. LangChain은 개발자와 기업이 AI 기술을 보다 쉽게 접근하고 활용할 수 있도록 돕고자 합니다.\n",
      "\n",
      "LangSmith와 같은 제품은 개발자들이 AI 모델을 더 쉽게 만들고 관리할 수 있도록 설계되었습니다. 예를 들어, 다음과 같은 시나리오를 상상해 보십시오:\n",
      "\n",
      "- 챗봇 개발: 랭스미스를 사용하여 자연어 이해(NLU) 모델을 선택하고, 챗봇의 답변 생성을 위한 템플릿을 만들고, 대화 흐름을 테스트하고 디버깅합니다.\n",
      "\n",
      "- 콘텐츠 생성: 랭체인을 활용하여 기사 제목을 생성하는 모델, 기사 초안을 작성하는 모델, 그리고 최종적으로 편집 및 교정 작업을 수행하는 모델을 연결하여 콘텐츠 생성 워크플로우를 자동화할 수 있습니다.\n",
      "\n",
      "이러한 도구들은 AI 개발 과정을 간소화하고, 개발자가 창의적이고 복잡한 AI 애플리케이션을 보다 빠르게 개발할 수 있도록 지원합니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \"LangChain의 Products(제품)는 어떤 것들이 있나요? 예를 들어 LangSmith 같은 Product가 있어어\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa937e0",
   "metadata": {},
   "source": [
    "#### Runnable의 stream() 함수 호출\n",
    "### 스트리밍 출력을 위한 요청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03fae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해 주세요.\"})\n",
    "    #스트리밍 출력\n",
    "    # print(answer)\n",
    "    # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "    for token in answer:\n",
    "        print(token, end = \"\", flush = True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5f2f2",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "* 첫번째 Chain의 출력이, 두번째 Chain의 입력이 된다.\n",
    "* 두 개의 Chain과 Prompt+OutputParser를 LCEL로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2f289bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 한국 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 10문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    # api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model = \"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49742ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1의 출력을 movie 입력력 변수로 전달\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "    response = chain2.invoke({\"genre\": \"액션\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0247c294",
   "metadata": {},
   "source": [
    "### PromptTemplate 여러 개 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    # api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model = \"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05e36ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 2 문장으로 요약해서 한국어로 답변해 주세요.', 'Gemma 모델의 학습 원리를 3 문장으로 요약해서 한국어로 답변해 주세요.', 'llama-4 모델의 학습 원리를 4 문장으로 요약해서 한국어로 답변해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama-4\", \"count\": 4},\n",
    "\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6609b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 모델은 대규모의 텍스트 데이터를 학습하여 언어 패턴과 구조를 익히는 방식으로 작동합니다. 이 모델은 주어진 문맥 내에서 다음에 올 단어를 예측하도록 훈련되며, 이를 통해 자연스러운 텍스트를 생성하거나 대화에 응답할 수 있습니다.\n",
      "Gemma는 컴퓨터가 자연어 처리를 더 잘하도록 설계된 언어 모델입니다. 텍스트의 통계적 패턴을 학습하여 문장 구조, 단어 의미 및 맥락을 이해합니다. 이 학습을 통해 Gemma는 질문에 답변하고, 텍스트를 요약하고, 새로운 텍스트를 생성하는 등 다양한 언어 관련 작업을 수행할 수 있습니다.\n",
      "LLama-4 모델은 Meta에서 개발한 대규모 언어 모델입니다. 이 모델은 수십억 개의 매개변수를 가지고 있으며, 방대한 양의 텍스트 데이터를 학습하여 자연어 처리 능력을 습득합니다. LLaMA-4의 학습 원리는 주어진 문맥에서 다음에 올 가능성이 높은 단어를 예측하도록 스스로 학습하는 자기 지도 학습 방식을 기반으로 합니다. 이를 통해 LLaMA-4는 다양한 자연어 처리 작업에 활용될 수 있는 강력한 언어 이해 능력을 개발할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    # api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model = \"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt) # AIMessage\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2423c82",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate \n",
    "* SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aafea846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "**딥러닝**은 인공신경망을 기반으로 하는 머신러닝의 한 분야입니다. 딥러닝은 데이터로부터 계층적인 패턴을 학습하여 문제를 해결하는 기술입니다. \n",
      "\n",
      "딥러닝은 사람의 뇌 구조를 모방한 인공신경망을 사용합니다. 이 인공신경망은 여러 개의 레이어로 구성되어 있으며, 각 레이어는 입력 데이터를 처리하여 다음 레이어로 전달합니다. 이러한 과정을 통해 딥러닝 모델은 데이터의 패턴을 학습하고, 예측이나 분류와 같은 작업을 수행할 수 있습니다.\n",
      "\n",
      "딥러닝의 핵심 개념은 다음과 같습니다.\n",
      "\n",
      "* **인공신경망**: 사람의 뇌 구조를 모방한 모델로, 여러 개의 레이어로 구성되어 있습니다.\n",
      "* **레이어**: 인공신경망을 구성하는 기본 단위로, 입력 데이터를 처리하여 다음 레이어로 전달합니다.\n",
      "* **활성화 함수**: 각 레이어에서 출력값을 계산하는 함수로, sigmoid, ReLU 등이 있습니다.\n",
      "* **손실 함수**: 모델의 성능을 평가하는 함수로, 예측값과 실제값의 차이를 계산합니다.\n",
      "* **최적화 알고리즘**: 모델의 파라미터를 업데이트하는 알고리즘으로, SGD, Adam 등이 있습니다.\n",
      "\n",
      "딥러닝은 다양한 분야에서 활용되고 있습니다. 예를 들어, 이미지 인식, 자연어 처리, 음성 인식, 자율 주행 자동차 등이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"당신은 {topic} 전문가입니다. 명확하고 자세하게 설명해주세요.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"딥러닝은 무엇인가요?\")\n",
    "\n",
    "# LLM 호출\n",
    "llm = ChatOpenAI(\n",
    "    # api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model = \"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c73a971",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "* 예시를 제공 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a9802c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 태양계의 행성\n",
      "1. **수성**: 가장 작은 행성으로 태양과 가깝습니다.\n",
      "2. **금성**: 밝고 뜨거운 행성입니다.\n",
      "3. **지구**: 생명체가 사는 유일한 행성입니다.\n",
      "4. **화성**: 붉은 행성으로 로봇 탐사가 활발합니다.\n",
      "5. **목성**: 태양계에서 가장 큰 행성입니다.\n",
      "6. **토성**: 아름다운 고리를 가진 행성입니다.\n",
      "7. **천왕성**: 자전축이 기울어진 행성입니다.\n",
      "8. **해왕성**: 가장 먼 행성으로 매우 춥습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"뉴턴의 운동 법칙을 요약해 주세요.\",\n",
    "        \"output\": \"\"\"### 뉴턴의 운동 법칙\n",
    "1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\n",
    "2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\n",
    "3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"지구의 대기 구성 요소를 알려주세요.\",\n",
    "        \"output\": \"\"\"### 지구 대기의 구성\n",
    "- **질소 (78%)**: 대기의 대부분을 차지합니다.\n",
    "- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\n",
    "- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\n",
    "- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate 적용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 구성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 초등학생도 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 생성 및 체인 구성\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "model = ChatOpenAI(\n",
    "    # api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model = \"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# 테스트 실행\n",
    "result = chain.invoke({\"input\": \"태양계의 행성들을 간략히 정리해 주세요.\"})\n",
    "#result = chain.invoke({\"input\": \"양자 얽힘이 무엇인가요?\"})\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
