{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffa0d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf91fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " '\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a modular, end-to-end language model training framework '\n",
      " 'designed to simplify the process of building large-scale language models. It '\n",
      " 'consists of a series of components, including data preprocessors, language '\n",
      " 'model processors, inference transformers, and postprocessors, which work '\n",
      " 'together to enable the creation of robust language models.\\n'\n",
      " '\\n'\n",
      " '### Key Components:\\n'\n",
      " '1. **Data Preprocessor**: Handles the loading, processing, and formatting of '\n",
      " 'raw input data (e.g., text, JSON, XML) into a format suitable for training.\\n'\n",
      " '2. **Language Model Processor**: Implements the actual language modeling '\n",
      " 'task using a pre-trained or custom model to generate, extrapolate, or '\n",
      " 'predict human-level language understanding.\\n'\n",
      " '3. **Inference Transformer**: Allows for real-time inference of language '\n",
      " 'models by providing interpretable outputs that explain how the model arrived '\n",
      " 'at its predictions.\\n'\n",
      " '4. **PostProcessor**: Enhances the output of the language model, enabling it '\n",
      " 'to produce results in a form that is useful for downstream applications.\\n'\n",
      " '\\n'\n",
      " '### Training Flow:\\n'\n",
      " 'LangChain processes input data through the data preprocessor into feature '\n",
      " 'vectors. The feature vectors are then passed to the language model '\n",
      " 'processor, which generates outputs. These outputs are finally processed by '\n",
      " 'the inference transformer, and the postprocessor converts these outputs into '\n",
      " 'the desired format or explains them in detail.\\n'\n",
      " '\\n'\n",
      " '### Use Cases:\\n'\n",
      " '- **Text Generation**: Creating coherent text from prompts (e.g., chatbots, '\n",
      " 'writing applications).\\n'\n",
      " '- **Summarization**: Summarizing documents or collections of documents.\\n'\n",
      " '- **Question Answering**: Answering questions posed by users based on a '\n",
      " 'database of knowledge.\\n'\n",
      " '- **Recommendation Systems**: Generating suggestions for products, services, '\n",
      " 'or content.\\n'\n",
      " '\\n'\n",
      " '### Advantages:\\n'\n",
      " '- **End-to-end**: Models are trained directly from raw text data without '\n",
      " 'requiring intermediate representations (e.g., embeddings).\\n'\n",
      " '- **Flexibility**: Allows for customization of components to better suit '\n",
      " 'specific use cases.\\n'\n",
      " '- **Efficiency**: Simplifies the training process by abstracting away many '\n",
      " 'steps of preprocessing and inference.\\n'\n",
      " '\\n'\n",
      " '### Challenges:\\n'\n",
      " '- **Interpretability**: While inference transformers provide interpretable '\n",
      " 'outputs, they still require a certain level of understanding of how the '\n",
      " 'model arrived at its predictions.\\n'\n",
      " '- **Component Coordination**: Ensuring that all components work together '\n",
      " 'seamlessly can be complex, especially when different component types need to '\n",
      " 'interact with each other in ways that are not predefined.\\n'\n",
      " '\\n'\n",
      " '### Example Workflow:\\n'\n",
      " '1. **Data Loading**: Load and preprocess raw text data (e.g., a paragraph of '\n",
      " 'text) into feature vectors.\\n'\n",
      " '2. **Feature Processing**: Transform the raw text into appropriate features '\n",
      " 'for training.\\n'\n",
      " '3. **Model Training**: Feed these features through the language model '\n",
      " 'processor to generate outputs.\\n'\n",
      " '4. **Inference**: Use the inference transformer to convert the generated '\n",
      " 'outputs into final form (e.g., human-readable text).\\n'\n",
      " '5. **Post-Processing**: Enhance or explain the outputs based on user needs.\\n'\n",
      " '\\n'\n",
      " 'LangChain is particularly useful for tasks that require seamless integration '\n",
      " 'of multiple components and components that need to process and interpret '\n",
      " 'their results in a human-readable format.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ deepseek-r1:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bdc6dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, and they want a detailed answer in Korean. Let me start by recalling the basics of Python.\n",
      "\n",
      "First, Python is a programming language. I should mention that it's known for being easy to learn and versatile. Maybe talk about its syntax being clean and readable. Oh, right, it was created by Guido van Rossum in 1991. I should include that.\n",
      "\n",
      "Then, the user might be interested in its uses. Python is used in web development, data analysis, AI, machine learning, automation, and more. I should list some key areas. Also, mention that it's open-source and has a large community, which is important for support and resources.\n",
      "\n",
      "I need to explain the features like interpreted language, dynamic typing, garbage collection, and the extensive standard library. Maybe add something about the Python ecosystem with frameworks like Django and libraries like NumPy, Pandas, Matplotlib.\n",
      "\n",
      "Wait, should I mention the syntax? Like indentation versus braces? That's a key point. Also, the use of colons in control structures. Maybe include an example, like a simple \"Hello, World!\" program.\n",
      "\n",
      "I should structure the answer in a logical flow: introduction, features, uses, and maybe a conclusion. Make sure it's detailed but not too technical. Avoid jargon where possible. Check if there are any recent developments in Python, like Python 3.x or new features in recent versions. But since the user didn't specify, maybe stick to the basics.\n",
      "\n",
      "Also, confirm that Python is a high-level language, which means it abstracts away some of the lower-level details. That's a good point to highlight. Maybe mention that it's cross-platform, so it works on different operating systems.\n",
      "\n",
      "I should avoid any markdown and keep the answer in Korean. Let me put it all together now.\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì€ **í”„ë¡œê·¸ë˜ë° ì–¸ì–´**ë¡œ, 1991ë…„ì— **Guido van Rossum**ê°€ ë§Œë“¤ì–´ë‚¸ ì–¸ì–´ì…ë‹ˆë‹¤. íŒŒì´ì¬ì€ **ê°„ê²°í•˜ê³  ëª…í™•í•œ ë¬¸ë²•**ì„ íŠ¹ì§•ìœ¼ë¡œ í•˜ë©°, **ë‹¤ì´ë‚˜ë¯¹ íƒ€ì…**ê³¼ ** garbage collection** ê¸°ëŠ¥ì„ ê°–ì¶”ì–´ **ê°„ë‹¨í•œ ì½”ë“œë¡œ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì œê³µ**í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ”§ ì£¼ìš” íŠ¹ì§•\n",
      "1. **ê°„ê²°í•œ ë¬¸ë²•**:  \n",
      "   - ë“¤ì—¬ì“°ê¸°(ì¸ë´íŠ¸)ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•´, **ì½”ë“œì˜ ê°€ë…ì„±**ì„ ë†’ì…ë‹ˆë‹¤.  \n",
      "   - `if`, `for`, `while` ë“±ì˜ êµ¬ë¬¸ì€ `:`ë¡œ ëë‚´ë©°, **ê°„ê²°í•œ êµ¬ì¡°**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **ë‹¤ë¥¸ ì–¸ì–´ì™€ì˜ ì°¨ì´**:  \n",
      "   - **C/C++**ì²˜ëŸ¼ ë³€ìˆ˜ íƒ€ì…ì„ ëª…ì‹œí•´ì•¼ í•˜ì§€ë§Œ, **Pythonì€ íƒ€ì…ì„ ìë™ìœ¼ë¡œ ì¶”ë¡ **í•©ë‹ˆë‹¤.  \n",
      "   - **JVM ê¸°ë°˜ ì–¸ì–´**(Java, C#)ì²˜ëŸ¼ **ëŸ°íƒ€ì„ì— ì‹¤í–‰**í•˜ì§€ë§Œ, **íŒŒì´ì¬ì€ ìŠ¤íƒ€ì¼ ì–¸ì–´**ë¡œ, **ì½”ë“œë¥¼ ì¦‰ì‹œ ì‹¤í–‰**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ë‹¤ì–‘í•œ í™œìš© ë¶„ì•¼**:  \n",
      "   - **ì›¹ ê°œë°œ**(Django, Flask), **ë°ì´í„° ë¶„ì„**(Pandas, NumPy), **ë¨¸ì‹ ëŸ¬ë‹**(Scikit-learn), **ìë™í™”**(automation), **ëª¨ë°”ì¼ ê°œë°œ**(Kotlinê³¼ í˜¸í™˜), **ì œì–´ ì‹œìŠ¤í…œ**, **ë¹„ë””ì˜¤ ê²Œì„ ê°œë°œ** ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "4. **í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°**:  \n",
      "   - **Open Source**ë¡œ, **í¬ë¡œìŠ¤ í”Œë«í¼**(Windows, macOS, Linux, iOS, Android)ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, **ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬**(ì˜ˆ: Django, TensorFlow, Matplotlib)ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **Python 3.x ë²„ì „**:  \n",
      "   - 2020ë…„ë¶€í„° **Python 3.x**ê°€ ê³µì‹ì ìœ¼ë¡œ ë°œí‘œë˜ë©°, **ê¸°ì¡´ Python 2.x**ì™€ í˜¸í™˜ë˜ì§€ ì•Šì§€ë§Œ, **ì›ë˜ì˜ ë¬¸ë²•ì€ ìœ ì§€**í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ“Œ ì˜ˆì‹œ ì½”ë“œ\n",
      "```python\n",
      "# \"Hello, World!\"ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œ\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ“š ì°¸ê³  ìë£Œ\n",
      "- **Python ê³µì‹ ì‚¬ì´íŠ¸**: https://docs.python.org/  \n",
      "- **Python ë¼ì´ë¸ŒëŸ¬ë¦¬**: https://pypi.org/  \n",
      "- **Python ê¸°ì´ˆ êµìœ¡**: https://www.python.org/learn/\n",
      "\n",
      "---\n",
      "\n",
      "íŒŒì´ì¬ì€ **ê°œë°œìì—ê²Œ ì¹œí™”ì ì¸ ì–¸ì–´**ë¡œ, **ë°ì´í„° ê³¼í•™, AI, ì›¹ ê°œë°œ, ìë™í™”** ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ í™œìš©ë©ë‹ˆë‹¤. ğŸš€\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ llama3.2 ëª¨ë¸ ë¡œë“œ\n",
    "# llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? ìì„¸í•˜ê²Œ í•œê¸€ë¡œ ë‹µë³€í•´ ì¤˜\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44809382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To determine which number is larger, I will compare the whole numbers first.\n",
      "\n",
      "Both numbers have a 9 in the ones place, so they are equal up to that point.\n",
      "\n",
      "Next, I'll examine the tenths place. The decimal for 9.9 has a 9, while 9.11 has a 1. Since 9 is greater than 1, this indicates that 9.9 is larger.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Compare the Whole Numbers\n",
      "Both numbers have the same whole number part:\n",
      "- **9** in both cases.\n",
      "\n",
      "Since the whole numbers are equal, we need to look at the decimal parts.\n",
      "\n",
      "### Step 2: Compare the Tenths Place\n",
      "Look at the first digit after the decimal point (tenths place):\n",
      "- **9.9**: The tenths place is **9**\n",
      "- **9.11**: The tenths place is **1**\n",
      "\n",
      "Since **9** (from 9.9) is greater than **1** (from 9.11), we can conclude that **9.9** is larger.\n",
      "\n",
      "### Final Answer\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is bigger}}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e4a2332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger, I will compare the whole numbers first.\n",
       "\n",
       "Both numbers have a 9 in the ones place, so they are equal up to that point.\n",
       "\n",
       "Next, I'll examine the tenths place. The decimal for 9.9 has a 9, while 9.11 has a 1. Since 9 is greater than 1, this indicates that 9.9 is larger.\n",
       "\n",
       "Therefore, 9.9 is bigger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Compare the Whole Numbers\n",
       "Both numbers have the same whole number part:\n",
       "- **9** in both cases.\n",
       "\n",
       "Since the whole numbers are equal, we need to look at the decimal parts.\n",
       "\n",
       "### Step 2: Compare the Tenths Place\n",
       "Look at the first digit after the decimal point (tenths place):\n",
       "- **9.9**: The tenths place is **9**\n",
       "- **9.11**: The tenths place is **1**\n",
       "\n",
       "Since **9** (from 9.9) is greater than **1** (from 9.11), we can conclude that **9.9** is larger.\n",
       "\n",
       "### Final Answer\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is bigger}}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f8cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? Let me write them down to compare them.\n",
      "\n",
      "First, 9.9. That's the same as 9.90. And 9.11 is another decimal. So, when comparing them, I should look at the digits from left to right. \n",
      "\n",
      "Starting with the whole number part. Both numbers have 9 as the whole number. So that's equal. Then, the decimal part. The first number, 9.9, has a decimal part of 0.9. The second number, 9.11, has a decimal part of 0.11. \n",
      "\n",
      "Now, comparing the tenths place. The first number has 9 in the tenths place, and the second number has 1 in the tenths place. Since 9 is greater than 1, that means 9.9 is larger than 9.11. \n",
      "\n",
      "Wait, but let me make sure I didn't mix up the places. The tenths place is the first digit after the decimal. So 9.9 is 9.90, and 9.11 is 9.11. So yes, 9.90 is larger than 9.11 because the tenths digit 9 is bigger than 1. \n",
      "\n",
      "I think that's right. There's no need to go further because once the tenths place is different, the rest of the digits don't matter. So 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
      "**ì¦ëª…:**  \n",
      "- ë‘ ìˆ˜ì˜ ì •ìˆ˜ ë¶€ë¶„ì€ ëª¨ë‘ 9ì…ë‹ˆë‹¤.  \n",
      "- ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•˜ë©´, 9.9ëŠ” 0.9, 9.11ì€ 0.11ì…ë‹ˆë‹¤.  \n",
      "- ì†Œìˆ˜ ë¶€ë¶„ì—ì„œ 9 > 1ì´ë¯€ë¡œ, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
      "\n",
      "**ë‹µ:** 9.9ê°€ ë” í° ìˆ˜ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "\n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "# model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "089691b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? Let me write them down to compare them.\n",
       "\n",
       "First, 9.9. That's the same as 9.90. And 9.11 is another decimal. So, when comparing them, I should look at the digits from left to right. \n",
       "\n",
       "Starting with the whole number part. Both numbers have 9 as the whole number. So that's equal. Then, the decimal part. The first number, 9.9, has a decimal part of 0.9. The second number, 9.11, has a decimal part of 0.11. \n",
       "\n",
       "Now, comparing the tenths place. The first number has 9 in the tenths place, and the second number has 1 in the tenths place. Since 9 is greater than 1, that means 9.9 is larger than 9.11. \n",
       "\n",
       "Wait, but let me make sure I didn't mix up the places. The tenths place is the first digit after the decimal. So 9.9 is 9.90, and 9.11 is 9.11. So yes, 9.90 is larger than 9.11 because the tenths digit 9 is bigger than 1. \n",
       "\n",
       "I think that's right. There's no need to go further because once the tenths place is different, the rest of the digits don't matter. So 9.9 is bigger than 9.11.\n",
       "</think>\n",
       "\n",
       "9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
       "**ì¦ëª…:**  \n",
       "- ë‘ ìˆ˜ì˜ ì •ìˆ˜ ë¶€ë¶„ì€ ëª¨ë‘ 9ì…ë‹ˆë‹¤.  \n",
       "- ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•˜ë©´, 9.9ëŠ” 0.9, 9.11ì€ 0.11ì…ë‹ˆë‹¤.  \n",
       "- ì†Œìˆ˜ ë¶€ë¶„ì—ì„œ 9 > 1ì´ë¯€ë¡œ, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
       "\n",
       "**ë‹µ:** 9.9ê°€ ë” í° ìˆ˜ì…ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119946f",
   "metadata": {},
   "source": [
    "### LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ DeepSeek ëª¨ë¸(ì¶”ë¡ )ê³¼ Qwen ëª¨ë¸(í•œê¸€ì‘ë‹µ)ì„ ì—°ë™í•˜ê¸°\n",
    "* poetry add langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "169a89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.5\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\\n\\n        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\\n        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\\n        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\\n        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\\n\\n        ì§€ì¹¨:\\n        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\\n        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\\n        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\\n        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\\n\\n        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        ì§ˆë¬¸: {question}\\n        ì¶”ë¡ : {thinking}\\n        '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#ì¶”ë¡  ëª¨ë¸\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "#ì‘ë‹µ ëª¨ë¸(í•œê¸€ì²˜ë¦¬ ê°€ëŠ¥)\n",
    "# generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "\n",
    "print(generation_model)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        ì§ˆë¬¸: {question}\n",
    "        ì¶”ë¡ : {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f599ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers are decimals. The whole numbers are the same, 9. So I need to look at the decimal parts.\n",
      "\n",
      "First, I should write them with the same number of decimal places to compare accurately. So 9.9 is the same as 9.90. Now comparing 9.90 and 9.11.\n",
      "\n",
      "Starting from the left, both have 9 in the units place. Then the tenths place: 9 vs 1. Since 9 is greater than 1, 9.90 is larger than 9.11. So the answer is 9.9.\n",
      "\n",
      "Wait, the user provided an inference that I need to use, but the original response didn't mention it. The assistant's task is to include the inference without explicitly stating it. So I need to make sure the answer is based on the given reasoning but not mention the inference step.\n",
      "\n",
      "So the answer is straightforward: 9.9 is larger because the tenths place of 9.9 is 9, which is greater than 1 in 9.11. The key is aligning the decimals and comparing each digit step by step.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ê°™ìœ¼ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¾¸ë©´, 9.90ê³¼ 9.11ì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ì—ì„œ 9 vs 1ì„ ë¹„êµí•˜ë©´, 9ê°€ ë” í´ ê²ƒì´ë©°, ë”°ë¼ì„œ 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ«ìì…ë‹ˆë‹¤.\n",
      "{'question': '9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\\n\\nNow, comparing each digit from left to right:\\n\\n- The units place for both numbers is 9.\\n- In the tenths place, 9 has a 9 and 9.11 has a 1.\\n  \\nSince 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': \"<think>\\nOkay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers are decimals. The whole numbers are the same, 9. So I need to look at the decimal parts.\\n\\nFirst, I should write them with the same number of decimal places to compare accurately. So 9.9 is the same as 9.90. Now comparing 9.90 and 9.11.\\n\\nStarting from the left, both have 9 in the units place. Then the tenths place: 9 vs 1. Since 9 is greater than 1, 9.90 is larger than 9.11. So the answer is 9.9.\\n\\nWait, the user provided an inference that I need to use, but the original response didn't mention it. The assistant's task is to include the inference without explicitly stating it. So I need to make sure the answer is based on the given reasoning but not mention the inference step.\\n\\nSo the answer is straightforward: 9.9 is larger because the tenths place of 9.9 is 9, which is greater than 1 in 9.11. The key is aligning the decimals and comparing each digit step by step.\\n</think>\\n\\n9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” **9.9**ì…ë‹ˆë‹¤.  \\në‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ê°™ìœ¼ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \\n9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¾¸ë©´, 9.90ê³¼ 9.11ì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \\nì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ì—ì„œ 9 vs 1ì„ ë¹„êµí•˜ë©´, 9ê°€ ë” í´ ê²ƒì´ë©°, ë”°ë¼ì„œ 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ«ìì…ë‹ˆë‹¤.\"}\n",
      "==> ìƒì„±ëœ ë‹µë³€: \n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers are decimals. The whole numbers are the same, 9. So I need to look at the decimal parts.\n",
      "\n",
      "First, I should write them with the same number of decimal places to compare accurately. So 9.9 is the same as 9.90. Now comparing 9.90 and 9.11.\n",
      "\n",
      "Starting from the left, both have 9 in the units place. Then the tenths place: 9 vs 1. Since 9 is greater than 1, 9.90 is larger than 9.11. So the answer is 9.9.\n",
      "\n",
      "Wait, the user provided an inference that I need to use, but the original response didn't mention it. The assistant's task is to include the inference without explicitly stating it. So I need to make sure the answer is based on the given reasoning but not mention the inference step.\n",
      "\n",
      "So the answer is straightforward: 9.9 is larger because the tenths place of 9.9 is 9, which is greater than 1 in 9.11. The key is aligning the decimals and comparing each digit step by step.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ê°™ìœ¼ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¾¸ë©´, 9.90ê³¼ 9.11ì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ì—ì„œ 9 vs 1ì„ ë¹„êµí•˜ë©´, 9ê°€ ë” í´ ê²ƒì´ë©°, ë”°ë¼ì„œ 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ«ìì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "#LangGraphì—ì„œ State ì‚¬ìš©ìì •ì˜ í´ë˜ìŠ¤ëŠ” ë…¸ë“œ ê°„ì˜ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” í‹€ì…ë‹ˆë‹¤. \n",
    "#ë…¸ë“œ ê°„ì— ê³„ì† ì „ë‹¬í•˜ê³  ì‹¶ê±°ë‚˜, ê·¸ë˜í”„ ë‚´ì—ì„œ ìœ ì§€í•´ì•¼ í•  ì •ë³´ë¥¼ ë¯¸ë¦¬ ì •ì˜í™ë‹ˆë‹¤. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "    \n",
    "#DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwenë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„°\n",
    "inputs = {\"question\": \"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"}\n",
    "\n",
    "# invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ í˜¸ì¶œ\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"==> ìƒì„±ëœ ë‹µë³€: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90fc1f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a0375a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "To make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. This means converting 9.9 into 9.90.\n",
      "\n",
      "Now that both numbers have two decimal places, I can directly compare them digit by digit from left to right.\n",
      "\n",
      "Starting with the units place: Both numbers have a 9 in the units place, so they are equal there.\n",
      "\n",
      "Next, looking at the tenths place: The first number has a 9, and the second number has a 1. Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "<think>\n",
      "Okay, let's tackle this question. The user is asking which number is bigger between 9.9 and 9.11. Hmm, so both numbers are decimals. I need to figure out which one is larger.\n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Now, moving to the decimal part. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \n",
      "\n",
      "So, comparing the tenths place: the first number has a 9, and the second has a 1. Since 9 is greater than 1, the first number is larger. Even though 9.90 is the same as 9.9, it's still bigger than 9.11 because the tenths place is higher. \n",
      "\n",
      "Wait, but the user might not know about the decimal places. Maybe they just see 9.9 and 9.11 and think 9.9 is bigger because it's longer. But actually, when comparing decimals, the position of each digit matters. So even though 9.9 has two decimal places, the tenths digit is 9, which is more than 1 in the tenths place of 9.11. \n",
      "\n",
      "So the conclusion is 9.9 is larger than 9.11. I should explain that by aligning the decimals and comparing each digit step by step. Make sure to clarify that the tenths place is where the difference lies. Also, mention that even though 9.9 has more decimal places, the higher tenths digit makes it bigger. \n",
      "\n",
      "I need to present this in a clear, conversational way without using markdown. Keep it simple and direct. Avoid technical jargon but ensure the explanation is thorough. Check if there's any other aspect to consider, like rounding or place value, but I think the main point is the tenths place. \n",
      "\n",
      "Yes, that's all. The key is to show the comparison step by step, highlight the difference in the tenths place, and confirm that 9.9 is indeed larger.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” 9.9ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ«ìëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ë™ì¼í•˜ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ í‘œí˜„í•˜ë©´, 9.11ì€ 9.11ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ ë¶€ë¶„ì—ì„œ ì²« ë²ˆì§¸ ìˆ«ìëŠ” 9, ë‘ ë²ˆì§¸ëŠ” 1ì…ë‹ˆë‹¤.  \n",
      "9ëŠ” 1ë³´ë‹¤ í¬ë¯€ë¡œ, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ«ìì…ë‹ˆë‹¤.  \n",
      "ì¦‰, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.<think>\n",
      "Okay, let's tackle this question. The user is asking which number is bigger between 9.9 and 9.11. Hmm, so both numbers are decimals. I need to figure out which one is larger.\n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Now, moving to the decimal part. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \n",
      "\n",
      "So, comparing the tenths place: the first number has a 9, and the second has a 1. Since 9 is greater than 1, the first number is larger. Even though 9.90 is the same as 9.9, it's still bigger than 9.11 because the tenths place is higher. \n",
      "\n",
      "Wait, but the user might not know about the decimal places. Maybe they just see 9.9 and 9.11 and think 9.9 is bigger because it's longer. But actually, when comparing decimals, the position of each digit matters. So even though 9.9 has two decimal places, the tenths digit is 9, which is more than 1 in the tenths place of 9.11. \n",
      "\n",
      "So the conclusion is 9.9 is larger than 9.11. I should explain that by aligning the decimals and comparing each digit step by step. Make sure to clarify that the tenths place is where the difference lies. Also, mention that even though 9.9 has more decimal places, the higher tenths digit makes it bigger. \n",
      "\n",
      "I need to present this in a clear, conversational way without using markdown. Keep it simple and direct. Avoid technical jargon but ensure the explanation is thorough. Check if there's any other aspect to consider, like rounding or place value, but I think the main point is the tenths place. \n",
      "\n",
      "Yes, that's all. The key is to show the comparison step by step, highlight the difference in the tenths place, and confirm that 9.9 is indeed larger.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” 9.9ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ«ìëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ë™ì¼í•˜ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ í‘œí˜„í•˜ë©´, 9.11ì€ 9.11ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ ë¶€ë¶„ì—ì„œ ì²« ë²ˆì§¸ ìˆ«ìëŠ” 9, ë‘ ë²ˆì§¸ëŠ” 1ì…ë‹ˆë‹¤.  \n",
      "9ëŠ” 1ë³´ë‹¤ í¬ë¯€ë¡œ, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ«ìì…ë‹ˆë‹¤.  \n",
      "ì¦‰, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcec7c0",
   "metadata": {},
   "source": [
    "### 2ê°œì˜ ëª¨ë¸ì„ ì—°ë™í•œ ì½”ë“œë¥¼ Gradioë¥¼ ì‚¬ìš©í•˜ì—¬ UIë¡œ ì‹¤í–‰í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd7ac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ì…ë ¥ ì§ˆë¬¸: íŒŒì´ì¬ì´ë€\n",
      "[DEBUG] ì§ˆë¬¸ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: 9\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: íŒŒì´ì¬ì´ë€\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: 9\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: 661\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: íŒŒì´ì¬ì€ Pythonì´ë¼ëŠ” ì–¸ì–´ì˜ ì•½ì¹­ì…ë‹ˆë‹¤. ì´ëŠ” ì¸êµ¬ 200ë§Œ ëŒ€ì˜ ë¯¸êµ­ í…ìŠ¤í†  ë¼ì¸ì— ìœ„ì¹˜í•œ ê°œë°œìë“¤ì˜ ì´ë¦„ì—ì„œ ìœ ë˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "íŒŒì´ì¬ì€ ê°ì²´ ê¸°ë°˜ ë° í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ë³€ìˆ˜ì™€ í•¨ìˆ˜ë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆê²Œ ì„¤ê³„ëœ ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì½”ë“œ ì‘ì„±ì´ ê°„í¸í•˜ê²Œ ì´ë£¨ì–´ì§€ë©°, ë‹¤ì–‘í•œ ì¥ì¹˜ì™€ ì‹œìŠ¤í…œê³¼ í†µì‹ í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ë„ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "íŒŒì´ì¬ì€ ë°ì´í„° ë¶„ì„, ì›¹ ê°œë°œ, ì¸ê³µ ì§€ëŠ¥ ë“± ë§ì€ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìœ¼ë©°, ê°œë°œìë“¤ì˜ ê´€ì‹¬ì„ ëŒê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ê·¸ì˜ ê°„í¸í•œ ì½”ë“œ ì‘ì„± ê¸°ë²•ê³¼ ì—¬ëŸ¬ ì–¸ì–´ì™€ ì‹œìŠ¤í…œê³¼ì˜ ì—°ì‚° ëŠ¥ë ¥ ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "íŒŒì´ì¬ì€ 1980ë…„ëŒ€ì— ë§Œë“¤ì–´ì§„ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì¤‘ í•˜ë‚˜ë¡œ, ì§€ê¸ˆê¹Œì§€ë„ ë§ì€ ê°œë°œìë“¤ì´ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ê·¸ ì–¸ì–´ê°€ ê°„í¸í•˜ê²Œ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³ , ë‹¤ì–‘í•œ ê¸°ê¸°ì™€ ì‹œìŠ¤í…œì„ ì§€ì›í•œë‹¤ëŠ” ì  ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "íŒŒì´ì¬ì˜ í•µì‹¬ì ì¸ ê¸°ëŠ¥ì€ ëŒ€í‘œì ìœ¼ë¡œ ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°(OOP)ê³¼ í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°(FP)ì— ì§‘ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤. OOPëŠ” ë³€ìˆ˜ì™€ í•¨ìˆ˜ë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆê²Œ í•˜ë©°, FPëŠ” ì½”ë“œë¥¼ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\n",
      "\n",
      "íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ ì¥ì¹˜ì™€ ì‹œìŠ¤í…œì„ ì§€ì›í•˜ê³  ìˆìœ¼ë©°, ë°ì´í„° ë¶„ì„ê³¼ ì›¹ ê°œë°œ ë“±ì—ì„œ ë›°ì–´ë‚œ ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, íŒŒì´ì¬ì˜ ê°€ë²¼ìš´ ì½”ë“œ ì‘ì„± ê¸°ë²•ê³¼ ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ì‹œìŠ¤í…œì„ ì§€ì›í•˜ëŠ” ëŠ¥ë ¥ ë•ë¶„ì— ë§ì€ ì‚¬ëŒë“¤ì´ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì • (Jupyter ë…¸íŠ¸ë¶ í˜¸í™˜)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter í™˜ê²½ì—ì„œëŠ” reconfigure ëŒ€ì‹  í™˜ê²½ë³€ìˆ˜ë¡œ ì²˜ë¦¬\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter ë…¸íŠ¸ë¶ì´ë‚˜ ë‹¤ë¥¸ í™˜ê²½ì—ì„œëŠ” íŒ¨ìŠ¤\n",
    "    pass\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •: ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ê³¼ ë‹µë³€ ìƒì„±ì„ ìˆ˜í–‰\n",
    "# - reasoning_model: ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë‚®ìŒ, ì •í™•í•œ ë¶„ì„ìš©)\n",
    "# - generation_model: ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë†’ìŒ, ì°½ì˜ì  ì‘ë‹µìš©)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen2.5:1.5b\", \n",
    "    # model=\"qwen3:1.7b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ìƒíƒœ(State) ì •ì˜: ê·¸ë˜í”„ì—ì„œ ìƒíƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ë°ì´í„° êµ¬ì¡°\n",
    "class State(TypedDict):\n",
    "    question: str   # ì‚¬ìš©ìì˜ ì§ˆë¬¸\n",
    "    thinking: str   # ì¶”ë¡  ê²°ê³¼\n",
    "    answer: str     # ìµœì¢… ë‹µë³€\n",
    "\n",
    "# ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \n",
    "        ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ í•œêµ­ì–´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "        \n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ì¤‘ìš”: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"ì§ˆë¬¸: {question}\n",
    "        \n",
    "        ì¶”ë¡  ê³¼ì •: {thinking}\n",
    "        \n",
    "        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"ë¬¸ìì—´ì´ UTF-8ë¡œ ì œëŒ€ë¡œ ì¸ì½”ë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë³€í™˜\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # ë¬¸ìì—´ì´ì§€ë§Œ ì¸ì½”ë”© ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # ë¬¸ìì—´ì„ UTF-8ë¡œ ì¸ì½”ë”©í–ˆë‹¤ê°€ ë‹¤ì‹œ ë””ì½”ë”©í•˜ì—¬ ì •ë¦¬\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] ì…ë ¥ ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] ì§ˆë¬¸ íƒ€ì…: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    # thinking_content = ensure_utf8_string(response.content)\n",
    "    thinking_content = response.content\n",
    "\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5ë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±\n",
    "def generate(state: State):\n",
    "    # question = ensure_utf8_string(state[\"question\"])\n",
    "    # thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    question = state[\"question\"]\n",
    "    thinking = state[\"thinking\"]\n",
    "\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    # answer_content = ensure_utf8_string(response.content)\n",
    "    answer_content = response.content\n",
    "\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜: ìƒíƒœ(State) ê°„ì˜ íë¦„ì„ ì •ì˜\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "# def launch_gradio():\n",
    "#     iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "#     iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n",
    "    # launch_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d3bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
